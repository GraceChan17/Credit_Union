{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thefuzz in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.22.1)\n",
      "Requirement already satisfied: python-Levenshtein in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thefuzz) (3.10.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-Levenshtein) (0.26.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install thefuzz python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.66.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def memory_efficient_fuzzy_merge(file1_path, file2_path, output_path, key1, key2, \n",
    "                               chunk_size=500, threshold=80):\n",
    "    \"\"\"\n",
    "    Memory-efficient fuzzy matching and merging for large datasets\n",
    "    \n",
    "    Parameters:\n",
    "    file1_path (str): Path to first dta file\n",
    "    file2_path (str): Path to second dta file\n",
    "    output_path (str): Path where merged file will be saved\n",
    "    key1 (str): Column name in first dataset containing bank IDs\n",
    "    key2 (str): Column name in second dataset containing bank IDs\n",
    "    chunk_size (int): Number of rows to process at once\n",
    "    threshold (int): Minimum similarity score (0-100) to consider a match\n",
    "    \"\"\"\n",
    "    # First, create a mapping dictionary using only unique IDs\n",
    "    print(\"Reading unique IDs from second file...\")\n",
    "    df2_ids = pd.read_stata(file2_path, columns=[key2])[key2].unique()\n",
    "    df2_ids = pd.Series(df2_ids).astype(str)\n",
    "    \n",
    "    print(\"Creating mapping dictionary...\")\n",
    "    # Process df1 in chunks to create mapping\n",
    "    mapping = {}\n",
    "    chunk_iterator = pd.read_stata(file1_path, chunksize=chunk_size)\n",
    "    \n",
    "    for chunk in tqdm(chunk_iterator):\n",
    "        chunk_ids = chunk[key1].astype(str).unique()\n",
    "        for id1 in chunk_ids:\n",
    "            if id1 not in mapping:  # Only process if we haven't seen this ID before\n",
    "                match = process.extractOne(id1, df2_ids, scorer=fuzz.ratio)\n",
    "                if match and match[1] >= threshold:\n",
    "                    mapping[id1] = match[0]\n",
    "    \n",
    "    print(\"Processing and saving merged data...\")\n",
    "    # Now process the actual merge in chunks\n",
    "    first_chunk = True\n",
    "    chunk_iterator = pd.read_stata(file1_path, chunksize=chunk_size)\n",
    "    \n",
    "    for chunk in tqdm(chunk_iterator):\n",
    "        # Process chunk\n",
    "        chunk[key1] = chunk[key1].astype(str)\n",
    "        chunk['matched_id'] = chunk[key1].map(mapping)\n",
    "        \n",
    "        # Read only necessary rows from df2\n",
    "        matched_ids = chunk['matched_id'].dropna().unique()\n",
    "        if len(matched_ids) > 0:\n",
    "            df2_chunk = pd.read_stata(file2_path)\n",
    "            df2_chunk = df2_chunk[df2_chunk[key2].astype(str).isin(matched_ids)]\n",
    "            \n",
    "            # Merge\n",
    "            merged_chunk = pd.merge(\n",
    "                chunk,\n",
    "                df2_chunk,\n",
    "                left_on='matched_id',\n",
    "                right_on=key2,\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Remove temporary column\n",
    "            merged_chunk = merged_chunk.drop('matched_id', axis=1)\n",
    "            \n",
    "            # Save\n",
    "            if first_chunk:\n",
    "                merged_chunk.to_stata(output_path)\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                merged_chunk.to_stata(output_path, append=True)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk\n",
    "        if 'df2_chunk' in locals():\n",
    "            del df2_chunk\n",
    "        if 'merged_chunk' in locals():\n",
    "            del merged_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading unique IDs from second file...\n",
      "Creating mapping dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18572it [4:43:36,  1.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmemory_efficient_fuzzy_merge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile1_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Volumes/aae/users/zchen2365/update_uniqueid.dta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile2_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Volumes/aae/users/zchen2365/combined_necessary_uniqueid.dta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Volumes/aae/users/zchen2365/python/merged_file.dta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbank_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbank_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust this based on your available memory\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mmemory_efficient_fuzzy_merge\u001b[0;34m(file1_path, file2_path, output_path, key1, key2, chunk_size, threshold)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id1 \u001b[38;5;129;01min\u001b[39;00m chunk_ids:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m id1 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mapping:  \u001b[38;5;66;03m# Only process if we haven't seen this ID before\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m         match \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractOne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuzz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mand\u001b[39;00m match[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[1;32m     31\u001b[0m             mapping[id1] \u001b[38;5;241m=\u001b[39m match[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/thefuzz/process.py:261\u001b[0m, in \u001b[0;36mextractOne\u001b[0;34m(query, choices, processor, scorer, score_cutoff)\u001b[0m\n\u001b[1;32m    258\u001b[0m is_lowered \u001b[38;5;241m=\u001b[39m scorer \u001b[38;5;129;01min\u001b[39;00m _scorer_lowering\n\u001b[1;32m    260\u001b[0m query \u001b[38;5;241m=\u001b[39m _preprocess_query(query, processor)\n\u001b[0;32m--> 261\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractOne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_cutoff\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:842\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.extractOne\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:482\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.extractOne_dict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:304\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.extractOne_dict_f64\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/thefuzz/utils.py:22\u001b[0m, in \u001b[0;36mfull_process\u001b[0;34m(s, force_ascii)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_ascii:\n\u001b[1;32m     20\u001b[0m     s \u001b[38;5;241m=\u001b[39m ascii_only(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory_efficient_fuzzy_merge(\n",
    "    file1_path='/Volumes/aae/users/zchen2365/update_uniqueid.dta',\n",
    "    file2_path='/Volumes/aae/users/zchen2365/combined_necessary_uniqueid.dta',\n",
    "    output_path='/Volumes/aae/users/zchen2365/python/merged_file.dta',\n",
    "    key1='bank_id',\n",
    "    key2='bank_id',\n",
    "    chunk_size=500,  # Adjust this based on your available memory\n",
    "    threshold=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
